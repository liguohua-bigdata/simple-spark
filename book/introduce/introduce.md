#一、spark简介
##1.官网地址
```
http://spark.apache.org
```
##2.Github地址
```
https://github.com/apache/spark
git://git.apache.org/spark.git
```
#二、spark特性
##1.处理速度快
![](images/logistic-regression.png) 
```
1.spark是基于内存的分布式计算框架，它比基于磁盘的MapReduce快的多。
2.spark在job执行时会根据任务关系的DAG图进行计算优化。
```
##2.通用性好
![](images/spark-stack.png) 
```
1.spark的软件栈比较丰富，这些软件栈能够处理大量的业务场景。
2.spark支持批处理，流处理，sql,图计算，机器学习等。
```
##3.集成性好
![](images/spark-runs-everywhere.png) 
```
1.spark和大数据生态圈中的其他技术集成性好，因此你可以把Spark和其他的大数据技术进行融合。
2.spark支持多种存储系统，多种调度系统，多种语言，因此spark的应用范围比较广泛。
```
#三、spark架构
![](images/cluster-overview.png) 
```
1.spark和Hadoop一样是一个主从式分布式的通用内存计算框架。
2.主节点充当ClusterManager的角色，负责分发任务，并监控从节点上的任务执行情况。
  2.1在standalone模式下，ClusterManager就spark的master节点。
  2.2在sparkOnYarn模式下，ClusterManager就是yarn的resourceManager
  2.3在sparkOnMesos模式下，ClusterManager就是mesosMaster。
3.从节点充当worker的角色，负责执行任务，并报任务进度给主节点。
  3.1worker中有一个或多executor进程，每个executor有一个cache用于缓存数据。
  3.2每个executor有一个或多个Task线程，复制具体执行分布式任务。
4.Driver在spark中是用来启动具体的application，它一般包含一个sparkContext用于
  表示spark执行上下文，通过sparkContext可以做一些设置操作。
```
![](images/Snip20161217_2.png) 
```
spark2.0中提出了sparkSession的概念，可以认为是对spark1.x中的sparkContex的封装。
通过sparkSession，我们可以使用sparkSQL的内容。
```

![](images/sparkapp-sparkcontext-master-slaves.png) 
